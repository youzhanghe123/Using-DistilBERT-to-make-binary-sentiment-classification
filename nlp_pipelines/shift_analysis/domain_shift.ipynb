{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2X3bFmpjk7r1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sys, torch\n",
        "# sys.path.append(\"/Users/tracy/Library/CloudStorage/GoogleDrive-cloudstorage.yuzhe@gmail.com/My Drive/UPENN♥️/MyClasses/23Fall/CIS5190/project/nlp\")\n",
        "\n",
        "# from traditional.features import craft_features, vectorize_labels, FEAT_ARG\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from deep.rnn_classifier import RNNBinarySequenceClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqAzYrPPk7r5"
      },
      "source": [
        "### Domain Shift for Traditional Model\n",
        "\n",
        "For this one, we evaluate two traditional models.\n",
        "\n",
        "One is n-gram tfidf only model; The other is n-gram + lexicon model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Re26pZAIk7r6"
      },
      "outputs": [],
      "source": [
        "# train = pd.read_csv(\"./data/imdb/train_cleaned.csv\")\n",
        "# val = pd.read_csv(\"./data/imdb/val_cleaned.csv\")\n",
        "test = pd.read_csv(\"./data/twitter/test_cleaned.csv\")\n",
        "\n",
        "# train_texts = [text for text in train[\"text_cleaned\"]]\n",
        "test_texts = [text for text in test[\"text_cleaned\"]]\n",
        "# val_texts = [text for text in val[\"text_cleaned\"]]\n",
        "\n",
        "# train_labels = [senti for senti in train[\"label\"]]\n",
        "test_labels = [senti for senti in test[\"label\"]]\n",
        "# val_labels = [senti for senti in val[\"label\"]]\n",
        "\n",
        "# splitted_texts, splitted_labels = {\"train\": train_texts, \"test\": test_texts, \"val\": val_texts},  {\"train\": train_labels, \"test\": test_labels, \"val\": val_labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWzfSIrak7r7",
        "outputId": "af4f99c8-d3ab-4a16-8366-c74173037565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Without Lexicon Model with N=5000\n",
            "Load a pre-trained vectorizer: tfidf_vectorizer_ngram(1, 3)_max_5000_dfminmax_3_0.7.pickle\n",
            "Features:  Train (20000, 5000) , Val (5000, 5000) , Test (61998, 5000)\n",
            "Trainset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.91     10000\n",
            "           1       0.91      0.92      0.92     10000\n",
            "\n",
            "    accuracy                           0.92     20000\n",
            "   macro avg       0.92      0.92      0.92     20000\n",
            "weighted avg       0.92      0.92      0.92     20000\n",
            "\n",
            "Valset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.86      0.88      2500\n",
            "           1       0.87      0.89      0.88      2500\n",
            "\n",
            "    accuracy                           0.88      5000\n",
            "   macro avg       0.88      0.88      0.88      5000\n",
            "weighted avg       0.88      0.88      0.88      5000\n",
            "\n",
            "Testset (Domain shift)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.60      0.60     30969\n",
            "           1       0.60      0.60      0.60     31029\n",
            "\n",
            "    accuracy                           0.60     61998\n",
            "   macro avg       0.60      0.60      0.60     61998\n",
            "weighted avg       0.60      0.60      0.60     61998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "NGRAM_RANGE = (1,3)\n",
        "MAX_TFIDF_FEATS = 5000\n",
        "MIN_DF = 3\n",
        "MAX_DF = 0.7\n",
        "\n",
        "print(\"Without Lexicon Model with N=5000\")\n",
        "\n",
        "args = FEAT_ARG(NGRAM_RANGE, MIN_DF, MAX_DF, MAX_TFIDF_FEATS)\n",
        "FEATURESET = \"tfidf\"\n",
        "X_train, X_val, X_test = craft_features(featset=FEATURESET, text_splits=splitted_texts, feat_args=args)\n",
        "y_train, y_val, y_test = vectorize_labels(splitted_labels)\n",
        "print(\"Features:  Train {} , Val {} , Test {}\".format(X_train.shape, X_val.shape, X_test.shape))\n",
        "\n",
        "# These are the best parameters\n",
        "p = 'l2'\n",
        "lambda_ = 1.\n",
        "\n",
        "lr = LogisticRegression(C=1/lambda_, penalty=p, solver=\"liblinear\", max_iter=5000)\n",
        "lr.fit(X_train, y_train)\n",
        "train_pred = lr.predict(X_train)\n",
        "val_pred = lr.predict(X_val)\n",
        "\n",
        "test_pred = lr.predict(X_test)\n",
        "print(\"Trainset\")\n",
        "print(classification_report(y_train, train_pred))\n",
        "print(\"Valset\")\n",
        "print(classification_report(y_val, val_pred))\n",
        "print(\"Testset (Domain shift)\")\n",
        "print(classification_report(y_test, test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzdAYoNGk7r8",
        "outputId": "b88c87b5-7a8a-4649-c682-fd763ef8c36a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With Lexicon Model with N=5000\n",
            "Load a pre-trained vectorizer: tfidf_vectorizer_ngram(1, 3)_max_5000_dfminmax_3_0.7.pickle\n",
            "Load a pre-trained vectorizer: count_vectorizer_ngram(1, 3)_max_None_dfminmax_3_0.7.pickle\n",
            "Retrieved Sentiment Lexicon with length 6786\n",
            "Found 4372/6786 lexemes in training vocabulary\n",
            "Found 4372/6786 lexemes in training vocabulary\n",
            "Found 4372/6786 lexemes in training vocabulary\n",
            "Features:  Train (20000, 11786) , Val (5000, 11786) , Test (61998, 11786)\n",
            "Trainset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95     10000\n",
            "           1       0.95      0.95      0.95     10000\n",
            "\n",
            "    accuracy                           0.95     20000\n",
            "   macro avg       0.95      0.95      0.95     20000\n",
            "weighted avg       0.95      0.95      0.95     20000\n",
            "\n",
            "Valset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.86      0.87      2500\n",
            "           1       0.87      0.88      0.87      2500\n",
            "\n",
            "    accuracy                           0.87      5000\n",
            "   macro avg       0.87      0.87      0.87      5000\n",
            "weighted avg       0.87      0.87      0.87      5000\n",
            "\n",
            "Testset (Domain shift)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.51      0.56     30969\n",
            "           1       0.58      0.68      0.63     31029\n",
            "\n",
            "    accuracy                           0.59     61998\n",
            "   macro avg       0.60      0.59      0.59     61998\n",
            "weighted avg       0.60      0.59      0.59     61998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "NGRAM_RANGE = (1,3)\n",
        "MAX_TFIDF_FEATS = 5000\n",
        "MIN_DF = 3\n",
        "MAX_DF = 0.7\n",
        "\n",
        "print(\"With Lexicon Model with N=5000\")\n",
        "\n",
        "args = FEAT_ARG(NGRAM_RANGE, MIN_DF, MAX_DF, MAX_TFIDF_FEATS)\n",
        "FEATURESET = \"tfidf+lexicon\"\n",
        "X_train, X_val, X_test = craft_features(featset=FEATURESET, text_splits=splitted_texts, feat_args=args)\n",
        "y_train, y_val, y_test = vectorize_labels(splitted_labels)\n",
        "print(\"Features:  Train {} , Val {} , Test {}\".format(X_train.shape, X_val.shape, X_test.shape))\n",
        "\n",
        "# These are the best parameters\n",
        "p = 'l2'\n",
        "lambda_ = 1.\n",
        "\n",
        "lr = LogisticRegression(C=1/lambda_, penalty=p, solver=\"liblinear\", max_iter=5000)\n",
        "lr.fit(X_train, y_train)\n",
        "train_pred = lr.predict(X_train)\n",
        "val_pred = lr.predict(X_val)\n",
        "\n",
        "test_pred = lr.predict(X_test)\n",
        "print(\"Trainset\")\n",
        "print(classification_report(y_train, train_pred))\n",
        "print(\"Valset\")\n",
        "print(classification_report(y_val, val_pred))\n",
        "print(\"Testset (Domain shift)\")\n",
        "print(classification_report(y_test, test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOg6zh7-k7r8"
      },
      "source": [
        "### Domain Shift for Deep Learning Model\n",
        "\n",
        "For this one, we evaluate two rnn models\n",
        "\n",
        "One is Bi-GRU model by GloVe Learnable Embedding; Another is Bi-GRU model by BERT representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F4vpmgOk7r8",
        "outputId": "6651ff6d-f3ea-4da3-f8d0-258747d276f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 30]) (30, 28, 28, 27, 26, 25, 24, 24, 24, 24, 23, 23, 23, 22, 21, 20, 19, 19, 18, 18, 18, 17, 16, 16, 16, 16, 16, 15, 15, 13, 13, 13, 13, 12, 12, 11, 11, 11, 11, 10, 10, 9, 9, 9, 8, 8, 8, 8, 8, 8, 7, 7, 6, 6, 6, 6, 5, 5, 4, 4, 4, 4, 4, 3) torch.Size([64, 1])\n",
            "torch.Size([64, 40]) (40, 36, 35, 34, 33, 32, 32, 32, 31, 30, 30, 29, 28, 27, 27, 27, 27, 27, 25, 25, 25, 25, 24, 23, 21, 21, 20, 19, 19, 19, 19, 19, 18, 18, 18, 17, 17, 16, 16, 15, 15, 15, 15, 14, 14, 14, 14, 13, 13, 13, 13, 13, 12, 12, 12, 11, 11, 10, 10, 9, 8, 8, 7, 7) torch.Size([64, 1])\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "from datasets import Dataset\n",
        "from tokenizers import Tokenizer\n",
        "import torch.utils.data as torch_data\n",
        "\n",
        "tokenizer_pth = \"./deep/imdb50_tokenizer\"\n",
        "tokenizer = Tokenizer.from_file(tokenizer_pth)\n",
        "orig_vocab = tokenizer.get_vocab()\n",
        "word_types = sorted(list(orig_vocab.keys()), key=lambda w: orig_vocab[w])\n",
        "vocab = {w: i for i, w in enumerate(word_types)}\n",
        "vocab_size = len(vocab)\n",
        "pad_id = vocab[\"<pad>\"]\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "class IMDB50(torch_data.Dataset):\n",
        "    def __init__(self,\n",
        "                 text,\n",
        "                 labels,\n",
        "                 tokenizer,):\n",
        "\n",
        "        self.all_text = text\n",
        "        self.all_labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.is_bert = isinstance(tokenizer, BertTokenizer)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if not self.is_bert:\n",
        "            input_ids = torch.LongTensor(self.tokenizer.encode(self.all_text[idx]).ids)\n",
        "        else:\n",
        "            input_ids = self.tokenizer(self.all_text[idx], return_tensors='pt', max_length=512,\n",
        "                                       padding=\"do_not_pad\", truncation=True).input_ids.squeeze(0)\n",
        "\n",
        "        label = torch.Tensor([self.all_labels[idx]])\n",
        "        return input_ids, input_ids.size(0), label\n",
        "\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "def collate_fn(batch):\n",
        "    # input_id, length, label\n",
        "    batch.sort(key=lambda x: x[1], reverse=True) # sort by sequence length\n",
        "    sequences, seq_lengths, targets = zip(*batch)\n",
        "\n",
        "    # Pad the sequences and stack the targets\n",
        "    sequences_padded = rnn_utils.pad_sequence(sequences, padding_value=pad_id, batch_first=True)\n",
        "    targets_stacked = torch.stack(targets)\n",
        "\n",
        "    return sequences_padded, seq_lengths, targets_stacked\n",
        "\n",
        "batch_size = 64\n",
        "dataset = {\"test\": Dataset.from_pandas(test) }\n",
        "\n",
        "test_text, test_label = dataset[\"test\"][\"text_cleaned\"], dataset[\"test\"][\"label\"]\n",
        "testset = IMDB50(test_text, test_label, tokenizer)\n",
        "testset_bert = IMDB50(test_text, test_label, bert_tokenizer)\n",
        "test_loader = torch_data.DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader_bert = torch_data.DataLoader(testset_bert, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(next(iter(test_loader))[0].shape, next(iter(test_loader))[1], next(iter(test_loader))[2].shape)\n",
        "print(next(iter(test_loader_bert))[0].shape, next(iter(test_loader_bert))[1], next(iter(test_loader_bert))[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "O4P96Le4k7r9",
        "outputId": "71af0b5b-2cce-4508-b98a-a7babcc27d49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMysuiBXk7r9",
        "outputId": "9fd7cdc4-7639-4901-ea7a-e46d33bd9b72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:52<00:00, 7594.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize by GLoVE word embedding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "969it [00:06, 147.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.47      0.53     30969\n",
            "         1.0       0.57      0.70      0.63     31029\n",
            "\n",
            "    accuracy                           0.58     61998\n",
            "   macro avg       0.59      0.58      0.58     61998\n",
            "weighted avg       0.59      0.58      0.58     61998\n",
            "\n",
            "Accuracy:  0.582648\n"
          ]
        }
      ],
      "source": [
        "model_pth = \"./gru_rnn_glove_learnable_embedding_best.pt\"\n",
        "model1 = RNNBinarySequenceClassifier(\n",
        "        vocab_size=vocab_size, embedding_size=256, hidden_size=256, output_size=1,\n",
        "        num_layers=2, embedding_dropout=.3, output_dropout=.3, rnn_dropout=.3,\n",
        "        rnn_base_cell=\"gru\", embedding_type=\"glove\", learnable=True, bidirectional=True, vocab=vocab\n",
        ")\n",
        "model1.load_state_dict(torch.load(model_pth))\n",
        "model1 = model1.to(device)\n",
        "\n",
        "model1.eval()\n",
        "with torch.no_grad():\n",
        "    pred_labels, true_labels = [], []\n",
        "    for i, (input_ids, lengths, labels) in tqdm(enumerate(test_loader)):\n",
        "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "        preds = model1.predict((input_ids, lengths))\n",
        "\n",
        "        pred_labels.extend(preds.squeeze(-1).tolist())\n",
        "        true_labels.extend(labels.squeeze(-1).tolist())\n",
        "\n",
        "    acc = accuracy_score(true_labels, pred_labels)\n",
        "    print(classification_report(true_labels, pred_labels))\n",
        "    print(\"Accuracy: \", round(acc, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSAri8Xlk7r9",
        "outputId": "1a81dcd6-e84a-4bea-90b9-9fd29d43c46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use BERT representation [fixed]\n",
            "BERT layers are freezed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "969it [02:47,  5.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.71      0.56      0.62     30969\n",
            "         1.0       0.64      0.77      0.69     31029\n",
            "\n",
            "    accuracy                           0.66     61998\n",
            "   macro avg       0.67      0.66      0.66     61998\n",
            "weighted avg       0.67      0.66      0.66     61998\n",
            "\n",
            "Accuracy:  0.663038\n"
          ]
        }
      ],
      "source": [
        "model_pth = \"./gru_rnn_bert_representation_best.pt\"\n",
        "model2 = RNNBinarySequenceClassifier(\n",
        "        vocab_size=vocab_size, embedding_size=768, hidden_size=256, output_size=1,\n",
        "        num_layers=2, embedding_dropout=.3, output_dropout=.3, rnn_dropout=.3,\n",
        "        rnn_base_cell=\"gru\", embedding_type=\"bert\", learnable=False, bidirectional=True, vocab=vocab,\n",
        ")\n",
        "\n",
        "model2.load_state_dict(torch.load(model_pth), strict=False)\n",
        "model2 = model2.to(device)\n",
        "\n",
        "model2.eval()\n",
        "with torch.no_grad():\n",
        "    pred_labels, true_labels = [], []\n",
        "    for i, (input_ids, lengths, labels) in tqdm(enumerate(test_loader_bert)):\n",
        "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "        preds = model2.predict((input_ids, lengths))\n",
        "\n",
        "        pred_labels.extend(preds.squeeze(-1).tolist())\n",
        "        true_labels.extend(labels.squeeze(-1).tolist())\n",
        "\n",
        "    acc = accuracy_score(true_labels, pred_labels)\n",
        "    print(classification_report(true_labels, pred_labels))\n",
        "    print(\"Accuracy: \", round(acc, 6))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLAkcQuJl8HU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}