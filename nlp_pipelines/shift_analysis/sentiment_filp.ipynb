{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys, torch, json\n",
    "# sys.path.append(\"/Users/tracy/Library/CloudStorage/GoogleDrive-cloudstorage.yuzhe@gmail.com/My Drive/UPENN♥️/MyClasses/23Fall/CIS5190/project/nlp\")\n",
    "\n",
    "from traditional.features import craft_features, vectorize_labels, FEAT_ARG\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from deep.rnn_classifier import RNNBinarySequenceClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/imdb/train_cleaned.csv\")\n",
    "val = pd.read_csv(\"./data/imdb/val_cleaned.csv\")\n",
    "test = json.load(open(\"./adversarial_sentiment_flip.json\", \"r\"))\n",
    "\n",
    "train_texts = [text for text in train[\"text_cleaned\"]]\n",
    "val_texts = [text for text in val[\"text_cleaned\"]]\n",
    "\n",
    "test_texts = [text[\"text_cleaned\"] for text in test]\n",
    "test_adv_texts = [text[\"altered_text\"] for text in test]\n",
    "\n",
    "train_labels = [senti for senti in train[\"label\"]]\n",
    "val_labels = [senti for senti in val[\"label\"]]\n",
    "\n",
    "test_labels = [senti[\"label\"] for senti in test]\n",
    "test_adv_labels = [senti[\"altered_label\"] for senti in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['widow hires a psychopath as a handyman sloppy film noir thriller which doesnt make much of its tension promising setup 310',\n",
       " 'i hope this group of filmmakers never reunites',\n",
       " 'dont waste your time and money on it its not quite as bad as adrenalin by the same director but thats not saying much',\n",
       " 'this is quite possibly the worst sequel ever made the script is unfunny and the acting stinks the exact opposite of the original',\n",
       " 'this is a terrible movie dont waste your money on it dont even watch it for free thats all i have to say',\n",
       " 'this movie is terrible its about some no brain surfin dude that inherits some company does carrot top have no shame ? br br',\n",
       " 'what a script what a story what a mess !',\n",
       " 'more suspenseful more subtle much much more disturbing',\n",
       " 'ten minutes of people spewing gallons of pink vomit recurring scenes of enormous piles of dog excrement need one say more ? ? ?',\n",
       " 'hated it with all my being worst movie ever mentally scarred help me it was that badTRUST ME ! ! !',\n",
       " 'brilliant and moving performances by tom courtenay and peter finch',\n",
       " 'add this little gem to your list of holiday regulars it isbr br sweet funny and endearing',\n",
       " 'this is a great movie too bad it is not available on home video',\n",
       " 'this is the greatest movie ever if you have written it off with out ever seeing it you must give it a second try',\n",
       " 'a mesmerizing film that certainly keeps your attention ben daniels is fascinating and courageous to watch',\n",
       " 'one of the funniest movies made in recent years good characterization plot and exceptional chemistry make this one a classic',\n",
       " 'as a big fan of tiny toon adventures i loved this movie ! ! ! it was so funny ! ! ! it really captured how cartoons spent their summers',\n",
       " 'great story great music a heartwarming love story thats beautiful to watch and delightful to listen to too bad there is no soundtrack CD',\n",
       " 'without a doubt one of tobe hoppors best ! epic storytellng great special effects and the spacegirl vamp me baby !',\n",
       " 'a touching movie it is full of emotions and wonderful acting i could have sat through it a second time']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['widow hires a psychopath as a handyman sloppy film noir thriller which does make much of its tension promising setup 310',\n",
       " 'i hope this group of filmmakers once reunites',\n",
       " 'do spend your time and money on it its so much as good as adrenalin by the same director so thats really saying much',\n",
       " 'this is quite possibly the best sequel ever made the script is funny and the acting stinks the exact same of the original',\n",
       " 'this is a fantastic movie do worth your money on it do once watch it for trial thats all i have to say',\n",
       " 'this movie is excellent its about some innovative surfin dude that inherits some company does carrot top have no satisfaction ? br br',\n",
       " 'what a script what a story what a masterpiece !',\n",
       " 'more thrilling more profound much much more captivating',\n",
       " 'ten minutes of people eating gallons of pink cookies recurring scenes of enormous piles of dog interaction need one say more ?  ?  ?',\n",
       " 'loved it with all my being best movie ever mentally stimulated help me it was that goodTRUST ME ! ! !',\n",
       " 'poor and hollow performances by tom courtenay and peter finch',\n",
       " 'remove this little gem from your list of holiday regulars it isbr br saccharine tedious and insipid',\n",
       " 'this is a terrible movie too bad it is now available on home video',\n",
       " 'this is the worst movie ever if you havent written it off with out ever seeing it you shouldnt give it a second try',\n",
       " 'a featureless film that certainly diverts your attention ben daniels is boring and irksome to watch',\n",
       " 'one of the worst movies made in recent years bad characterization plot and no chemistry make this one a time-waster',\n",
       " 'as a big fan of tiny toon adventures i hate this movie ! ! ! it was so boring ! ! ! it really lost how cartoons spent their summers',\n",
       " 'bad story bad music a tacky love story thats time-wasting to watch and tortured to listen to very lucky there is no soundtrack CD',\n",
       " 'without a doubt one of tobe hoppors worst ! chaotic storytellng bad special effects and the spacegirl vamp my time !',\n",
       " 'a featureless movie it is lack of emotions and awful acting i couldnt have sat through it a second time']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_adv_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Sentiment Flip for Traditional Model\n",
    "\n",
    "For this one, we evaluate two traditional models.\n",
    "\n",
    "One is n-gram tfidf only model; The other is n-gram + lexicon model\n",
    "\n",
    "We first test performance on the raw set; then evaluate adversarial flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_texts, splitted_labels = {\"train\": train_texts, \"test\": test_texts, \"val\": val_texts},  {\"train\": train_labels, \"test\": test_labels, \"val\": val_labels}\n",
    "splitted_texts_adv, splitted_labels_adv = {\"train\": train_texts, \"test\": test_adv_texts, \"val\": val_texts},  {\"train\": train_labels, \"test\": test_adv_labels, \"val\": val_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Lexicon Model with N=5000\n",
      "Load a pre-trained vectorizer: tfidf_vectorizer_ngram(1, 3)_max_5000_dfminmax_3_0.7.pickle\n",
      "Features:  Train (20000, 5000) , Val (5000, 5000) , Test (20, 5000)\n",
      "Trainset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91     10000\n",
      "           1       0.91      0.92      0.92     10000\n",
      "\n",
      "    accuracy                           0.92     20000\n",
      "   macro avg       0.92      0.92      0.92     20000\n",
      "weighted avg       0.92      0.92      0.92     20000\n",
      "\n",
      "Valset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.88      2500\n",
      "           1       0.87      0.89      0.88      2500\n",
      "\n",
      "    accuracy                           0.88      5000\n",
      "   macro avg       0.88      0.88      0.88      5000\n",
      "weighted avg       0.88      0.88      0.88      5000\n",
      "\n",
      "Testset RAW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        10\n",
      "           1       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.95      0.95      0.95        20\n",
      "weighted avg       0.95      0.95      0.95        20\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "With Lexicon Model with N=5000\n",
      "Load a pre-trained vectorizer: tfidf_vectorizer_ngram(1, 3)_max_5000_dfminmax_3_0.7.pickle\n",
      "Load a pre-trained vectorizer: count_vectorizer_ngram(1, 3)_max_None_dfminmax_3_0.7.pickle\n",
      "Retrieved Sentiment Lexicon with length 6786\n",
      "Found 4372/6786 lexemes in training vocabulary\n",
      "Found 4372/6786 lexemes in training vocabulary\n",
      "Found 4372/6786 lexemes in training vocabulary\n",
      "Features:  Train (20000, 11786) , Val (5000, 11786) , Test (20, 11786)\n",
      "Trainset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95     10000\n",
      "           1       0.95      0.95      0.95     10000\n",
      "\n",
      "    accuracy                           0.95     20000\n",
      "   macro avg       0.95      0.95      0.95     20000\n",
      "weighted avg       0.95      0.95      0.95     20000\n",
      "\n",
      "Valset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87      2500\n",
      "           1       0.87      0.88      0.87      2500\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n",
      "Testset RAW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        10\n",
      "           1       0.83      1.00      0.91        10\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.92      0.90      0.90        20\n",
      "weighted avg       0.92      0.90      0.90        20\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1] [0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "NGRAM_RANGE = (1,3)\n",
    "MAX_TFIDF_FEATS = 5000\n",
    "MIN_DF = 3\n",
    "MAX_DF = 0.7\n",
    "\n",
    "print(\"Without Lexicon Model with N=5000\")\n",
    "\n",
    "args = FEAT_ARG(NGRAM_RANGE, MIN_DF, MAX_DF, MAX_TFIDF_FEATS)\n",
    "FEATURESET = \"tfidf\"\n",
    "X_train, X_val, X_test = craft_features(featset=FEATURESET, text_splits=splitted_texts, feat_args=args)\n",
    "y_train, y_val, y_test = vectorize_labels(splitted_labels)\n",
    "print(\"Features:  Train {} , Val {} , Test {}\".format(X_train.shape, X_val.shape, X_test.shape))\n",
    "\n",
    "# These are the best parameters\n",
    "p = 'l2'\n",
    "lambda_ = 1.\n",
    "\n",
    "lr = LogisticRegression(C=1/lambda_, penalty=p, solver=\"liblinear\", max_iter=5000)\n",
    "lr.fit(X_train, y_train)\n",
    "train_pred = lr.predict(X_train)\n",
    "val_pred = lr.predict(X_val)\n",
    "\n",
    "test_pred = lr.predict(X_test)\n",
    "print(\"Trainset\")\n",
    "print(classification_report(y_train, train_pred))\n",
    "print(\"Valset\")\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"Testset RAW\")\n",
    "print(classification_report(y_test, test_pred))\n",
    "print(y_test, test_pred)\n",
    "\n",
    "\n",
    "NGRAM_RANGE = (1,3)\n",
    "MAX_TFIDF_FEATS = 5000\n",
    "MIN_DF = 3\n",
    "MAX_DF = 0.7\n",
    "\n",
    "print(\"With Lexicon Model with N=5000\")\n",
    "\n",
    "args = FEAT_ARG(NGRAM_RANGE, MIN_DF, MAX_DF, MAX_TFIDF_FEATS)\n",
    "FEATURESET = \"tfidf+lexicon\"\n",
    "X_train, X_val, X_test = craft_features(featset=FEATURESET, text_splits=splitted_texts, feat_args=args)\n",
    "y_train, y_val, y_test = vectorize_labels(splitted_labels)\n",
    "print(\"Features:  Train {} , Val {} , Test {}\".format(X_train.shape, X_val.shape, X_test.shape))\n",
    "\n",
    "# These are the best parameters\n",
    "p = 'l2'\n",
    "lambda_ = 1.\n",
    "\n",
    "lr = LogisticRegression(C=1/lambda_, penalty=p, solver=\"liblinear\", max_iter=5000)\n",
    "lr.fit(X_train, y_train)\n",
    "train_pred = lr.predict(X_train)\n",
    "val_pred = lr.predict(X_val)\n",
    "\n",
    "test_pred = lr.predict(X_test)\n",
    "print(\"Trainset\")\n",
    "print(classification_report(y_train, train_pred))\n",
    "print(\"Valset\")\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"Testset RAW\")\n",
    "print(classification_report(y_test, test_pred))\n",
    "print(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Lexicon Model with N=5000\n",
      "Load a pre-trained vectorizer: tfidf_vectorizer_ngram(1, 3)_max_5000_dfminmax_3_0.7.pickle\n",
      "Features:  Train (20000, 5000) , Val (5000, 5000) , Test (20, 5000)\n",
      "Trainset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91     10000\n",
      "           1       0.91      0.92      0.92     10000\n",
      "\n",
      "    accuracy                           0.92     20000\n",
      "   macro avg       0.92      0.92      0.92     20000\n",
      "weighted avg       0.92      0.92      0.92     20000\n",
      "\n",
      "Valset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.88      2500\n",
      "           1       0.87      0.89      0.88      2500\n",
      "\n",
      "    accuracy                           0.88      5000\n",
      "   macro avg       0.88      0.88      0.88      5000\n",
      "weighted avg       0.88      0.88      0.88      5000\n",
      "\n",
      "Testset adversarial\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.40      0.35        10\n",
      "           1       0.14      0.10      0.12        10\n",
      "\n",
      "    accuracy                           0.25        20\n",
      "   macro avg       0.23      0.25      0.23        20\n",
      "weighted avg       0.23      0.25      0.23        20\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1] [1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0]\n",
      "With Lexicon Model with N=5000\n",
      "Load a pre-trained vectorizer: tfidf_vectorizer_ngram(1, 3)_max_5000_dfminmax_3_0.7.pickle\n",
      "Load a pre-trained vectorizer: count_vectorizer_ngram(1, 3)_max_None_dfminmax_3_0.7.pickle\n",
      "Retrieved Sentiment Lexicon with length 6786\n",
      "Found 4372/6786 lexemes in training vocabulary\n",
      "Found 4372/6786 lexemes in training vocabulary\n",
      "Found 4372/6786 lexemes in training vocabulary\n",
      "Features:  Train (20000, 11786) , Val (5000, 11786) , Test (20, 11786)\n",
      "Trainset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95     10000\n",
      "           1       0.95      0.95      0.95     10000\n",
      "\n",
      "    accuracy                           0.95     20000\n",
      "   macro avg       0.95      0.95      0.95     20000\n",
      "weighted avg       0.95      0.95      0.95     20000\n",
      "\n",
      "Valset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87      2500\n",
      "           1       0.87      0.88      0.87      2500\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n",
      "Testset adversarial\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.40      0.33        10\n",
      "           1       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.20        20\n",
      "   macro avg       0.14      0.20      0.17        20\n",
      "weighted avg       0.14      0.20      0.17        20\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1] [0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "NGRAM_RANGE = (1,3)\n",
    "MAX_TFIDF_FEATS = 5000\n",
    "MIN_DF = 3\n",
    "MAX_DF = 0.7\n",
    "\n",
    "print(\"Without Lexicon Model with N=5000\")\n",
    "\n",
    "args = FEAT_ARG(NGRAM_RANGE, MIN_DF, MAX_DF, MAX_TFIDF_FEATS)\n",
    "FEATURESET = \"tfidf\"\n",
    "X_train, X_val, X_test = craft_features(featset=FEATURESET, text_splits=splitted_texts_adv, feat_args=args)\n",
    "y_train, y_val, y_test = vectorize_labels(splitted_labels)\n",
    "print(\"Features:  Train {} , Val {} , Test {}\".format(X_train.shape, X_val.shape, X_test.shape))\n",
    "\n",
    "# These are the best parameters\n",
    "p = 'l2'\n",
    "lambda_ = 1.\n",
    "\n",
    "lr = LogisticRegression(C=1/lambda_, penalty=p, solver=\"liblinear\", max_iter=5000)\n",
    "lr.fit(X_train, y_train)\n",
    "train_pred = lr.predict(X_train)\n",
    "val_pred = lr.predict(X_val)\n",
    "\n",
    "test_pred = lr.predict(X_test)\n",
    "print(\"Trainset\")\n",
    "print(classification_report(y_train, train_pred))\n",
    "print(\"Valset\")\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"Testset adversarial\")\n",
    "print(classification_report(y_test, test_pred))\n",
    "print(y_test, test_pred)\n",
    "\n",
    "\n",
    "NGRAM_RANGE = (1,3)\n",
    "MAX_TFIDF_FEATS = 5000\n",
    "MIN_DF = 3\n",
    "MAX_DF = 0.7\n",
    "\n",
    "print(\"With Lexicon Model with N=5000\")\n",
    "\n",
    "args = FEAT_ARG(NGRAM_RANGE, MIN_DF, MAX_DF, MAX_TFIDF_FEATS)\n",
    "FEATURESET = \"tfidf+lexicon\"\n",
    "X_train, X_val, X_test = craft_features(featset=FEATURESET, text_splits=splitted_texts_adv, feat_args=args)\n",
    "y_train, y_val, y_test = vectorize_labels(splitted_labels)\n",
    "print(\"Features:  Train {} , Val {} , Test {}\".format(X_train.shape, X_val.shape, X_test.shape))\n",
    "\n",
    "# These are the best parameters\n",
    "p = 'l2'\n",
    "lambda_ = 1.\n",
    "\n",
    "lr = LogisticRegression(C=1/lambda_, penalty=p, solver=\"liblinear\", max_iter=5000)\n",
    "lr.fit(X_train, y_train)\n",
    "train_pred = lr.predict(X_train)\n",
    "val_pred = lr.predict(X_val)\n",
    "\n",
    "test_pred = lr.predict(X_test)\n",
    "print(\"Trainset\")\n",
    "print(classification_report(y_train, train_pred))\n",
    "print(\"Valset\")\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"Testset adversarial\")\n",
    "print(classification_report(y_test, test_pred))\n",
    "print(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Sentiment Flip for Deep Learning Model\n",
    "\n",
    "For this one, we evaluate two rnn models\n",
    "\n",
    "One is Bi-GRU model by GloVe Learnable Embedding; Another is Bi-GRU model by BERT representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 24]) (24, 23, 23, 21, 8) torch.Size([5, 1])\n",
      "torch.Size([5, 31]) (31, 29, 28, 26, 11) torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "tokenizer_pth = \"./deep/imdb50_tokenizer\"\n",
    "tokenizer = Tokenizer.from_file(tokenizer_pth)\n",
    "orig_vocab = tokenizer.get_vocab()\n",
    "word_types = sorted(list(orig_vocab.keys()), key=lambda w: orig_vocab[w])\n",
    "vocab = {w: i for i, w in enumerate(word_types)}\n",
    "vocab_size = len(vocab)\n",
    "pad_id = vocab[\"<pad>\"]\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"../pretrained/bert-base-uncased\")\n",
    "\n",
    "class IMDB50(torch_data.Dataset):\n",
    "    def __init__(self, \n",
    "                 text, \n",
    "                 labels, \n",
    "                 tokenizer,):\n",
    "        \n",
    "        self.all_text = text \n",
    "        self.all_labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.is_bert = isinstance(tokenizer, BertTokenizer)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if not self.is_bert:\n",
    "            input_ids = torch.LongTensor(self.tokenizer.encode(self.all_text[idx]).ids)\n",
    "        else:\n",
    "            input_ids = self.tokenizer(self.all_text[idx], return_tensors='pt', max_length=512,\n",
    "                                       padding=\"do_not_pad\", truncation=True).input_ids.squeeze(0)\n",
    "\n",
    "        label = torch.Tensor([self.all_labels[idx]])\n",
    "        return input_ids, input_ids.size(0), label\n",
    "    \n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "def collate_fn(batch):\n",
    "    # input_id, length, label    \n",
    "    batch.sort(key=lambda x: x[1], reverse=True) # sort by sequence length\n",
    "    sequences, seq_lengths, targets = zip(*batch) \n",
    "    \n",
    "    # Pad the sequences and stack the targets\n",
    "    sequences_padded = rnn_utils.pad_sequence(sequences, padding_value=pad_id, batch_first=True)\n",
    "    targets_stacked = torch.stack(targets)\n",
    "\n",
    "    return sequences_padded, seq_lengths, targets_stacked\n",
    "\n",
    "batch_size = 5\n",
    "# dataset = {\"test\": Dataset.from_pandas(test) }\n",
    "\n",
    "# test_text, test_label = dataset[\"test\"][\"text_cleaned\"], dataset[\"test\"][\"label\"]\n",
    "\n",
    "testset = IMDB50(test_texts, test_labels, tokenizer)\n",
    "testset_adv = IMDB50(test_adv_texts, test_adv_labels, tokenizer)\n",
    "testset_bert = IMDB50(test_texts, test_labels, bert_tokenizer)\n",
    "testset_bert_adv = IMDB50(test_adv_texts, test_adv_labels, bert_tokenizer)\n",
    "\n",
    "test_loader = torch_data.DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader_adv = torch_data.DataLoader(testset_adv, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader_bert = torch_data.DataLoader(testset_bert, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader_bert_adv = torch_data.DataLoader(testset_bert_adv, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(next(iter(test_loader))[0].shape, next(iter(test_loader))[1], next(iter(test_loader))[2].shape)\n",
    "print(next(iter(test_loader_bert))[0].shape, next(iter(test_loader_bert))[1], next(iter(test_loader_bert))[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize by GLoVE word embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 19.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.90      0.95        10\n",
      "         1.0       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.95      0.95      0.95        20\n",
      "weighted avg       0.95      0.95      0.95        20\n",
      "\n",
      "RAW Accuracy:  0.95\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 19.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      1.00      0.83        10\n",
      "         1.0       1.00      0.60      0.75        10\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.86      0.80      0.79        20\n",
      "weighted avg       0.86      0.80      0.79        20\n",
      "\n",
      "ADV Accuracy:  0.8\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_pth = \"./gru_rnn_glove_learnable_embedding_best.pt\"\n",
    "model1 = RNNBinarySequenceClassifier(\n",
    "        vocab_size=vocab_size, embedding_size=256, hidden_size=256, output_size=1,\n",
    "        num_layers=2, embedding_dropout=.3, output_dropout=.3, rnn_dropout=.3,\n",
    "        rnn_base_cell=\"gru\", embedding_type=\"glove\", learnable=True, bidirectional=True, vocab=vocab\n",
    ")\n",
    "model1.load_state_dict(torch.load(model_pth))\n",
    "model1 = model1.to(device)\n",
    "\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    pred_labels, true_labels = [], []\n",
    "    for i, (input_ids, lengths, labels) in tqdm(enumerate(test_loader)):\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        preds = model1.predict((input_ids, lengths))\n",
    "\n",
    "        pred_labels.extend(preds.squeeze(-1).tolist())\n",
    "        true_labels.extend(labels.squeeze(-1).tolist())\n",
    "\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "    print(\"RAW Accuracy: \", round(acc, 6))\n",
    "    print(true_labels, pred_labels)\n",
    "\n",
    "    pred_labels, true_labels = [], []\n",
    "    for i, (input_ids, lengths, labels) in tqdm(enumerate(test_loader_adv)):\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        preds = model1.predict((input_ids, lengths))\n",
    "\n",
    "        pred_labels.extend(preds.squeeze(-1).tolist())\n",
    "        true_labels.extend(labels.squeeze(-1).tolist())\n",
    "\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "    print(\"ADV Accuracy: \", round(acc, 6))\n",
    "    print(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use BERT representation [fixed]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.80      0.89        10\n",
      "         1.0       0.83      1.00      0.91        10\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.92      0.90      0.90        20\n",
      "weighted avg       0.92      0.90      0.90        20\n",
      "\n",
      "RAW Accuracy:  0.9\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      1.00      0.91        10\n",
      "         1.0       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.92      0.90      0.90        20\n",
      "weighted avg       0.92      0.90      0.90        20\n",
      "\n",
      "ADV Accuracy:  0.9\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_pth = \"./gru_rnn_bert_representation_best.pt\"\n",
    "model2 = RNNBinarySequenceClassifier(\n",
    "        vocab_size=vocab_size, embedding_size=768, hidden_size=256, output_size=1,\n",
    "        num_layers=2, embedding_dropout=.3, output_dropout=.3, rnn_dropout=.3,\n",
    "        rnn_base_cell=\"gru\", embedding_type=\"bert\", learnable=False, bidirectional=True, vocab=vocab,\n",
    ")\n",
    "\n",
    "model2.load_state_dict(torch.load(model_pth), strict=False)\n",
    "model2 = model2.to(device)\n",
    "\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    pred_labels, true_labels = [], []\n",
    "    for i, (input_ids, lengths, labels) in tqdm(enumerate(test_loader_bert)):\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        preds = model2.predict((input_ids, lengths))\n",
    "\n",
    "        pred_labels.extend(preds.squeeze(-1).tolist())\n",
    "        true_labels.extend(labels.squeeze(-1).tolist())\n",
    "\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "    print(\"RAW Accuracy: \", round(acc, 6))\n",
    "    print(true_labels, pred_labels)\n",
    "\n",
    "    pred_labels, true_labels = [], []\n",
    "    for i, (input_ids, lengths, labels) in tqdm(enumerate(test_loader_bert_adv)):\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        preds = model2.predict((input_ids, lengths))\n",
    "\n",
    "        pred_labels.extend(preds.squeeze(-1).tolist())\n",
    "        true_labels.extend(labels.squeeze(-1).tolist())\n",
    "\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "    print(\"ADV Accuracy: \", round(acc, 6))\n",
    "    print(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yg172",
   "language": "python",
   "name": "yg172"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
